{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3XhAJ8EwTLn",
    "outputId": "9e373946-405e-4c1b-e964-db997c0baa80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
      "\u001b[K     |████████████████████████████████| 646 kB 4.3 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391638 sha256=5f4c6c4e35931017b6c5ecd70f138d3032bb0c195a9e596223f5c4292be9b6c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.0\n"
     ]
    }
   ],
   "source": [
    "pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZCsBOFpywWhz"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !unzip /content/drive/MyDrive/Datasets/Gallery.zip\n",
    "\n",
    "!unzip /content/drive/MyDrive/Datasets/Animal_classes_93.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNL2ag4lv99t",
    "outputId": "967beeca-96e4-493e-ade6-f50dc676c001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# For commands\n",
    "import os\n",
    "os.chdir('/content/')\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# For array manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.util.testing as tm\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import imageio as io\n",
    "from pylab import *\n",
    "from sklearn.manifold import TSNE\n",
    "#For model performance\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "#For model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "from keras.models import Sequential\n",
    "from PIL import Image \n",
    "import os \n",
    "from IPython.display import display\n",
    "from IPython.display import Image as _Imgd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.models import load_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNDRv5-3wuzd",
    "outputId": "890b78c2-dd6a-45b5-9e45-73e4c7535be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "train_path = '/content/Animal_classes_93/animals/animals'\n",
    "\n",
    "# VGG16 model imported\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YJxQ5DU11hk",
    "outputId": "01173715-ddbd-4021-ad83-daaf2439afb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 93 classes\n"
     ]
    }
   ],
   "source": [
    "folders = glob('/content/Animal_classes_93/animals/animals/*')\n",
    "print(\"Our training set has \"+str(len(folders)) + \" classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdzBSCnB15fb",
    "outputId": "38f38654-bcda-4d6c-e89c-aec336f29b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 93)                2333277   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,047,965\n",
      "Trainable params: 2,333,277\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QbYtlIEb2N5x"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf \n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5mTbAzY2Oa4",
    "outputId": "e23b6be9-0192-42c8-8293-dd3a6ccc2d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37544 images belonging to 93 classes.\n",
      "Found 9378 images belonging to 93 classes.\n",
      "Epoch 1/20\n",
      "1173/1173 [==============================] - 670s 570ms/step - loss: 1.4666 - accuracy: 0.7499 - val_loss: 2.5676 - val_accuracy: 0.6510\n",
      "Epoch 2/20\n",
      "1173/1173 [==============================] - 666s 568ms/step - loss: 0.9250 - accuracy: 0.8322 - val_loss: 2.4004 - val_accuracy: 0.6945\n",
      "Epoch 3/20\n",
      "1173/1173 [==============================] - 666s 568ms/step - loss: 0.7413 - accuracy: 0.8617 - val_loss: 2.8736 - val_accuracy: 0.6839\n",
      "Epoch 4/20\n",
      "1173/1173 [==============================] - 664s 566ms/step - loss: 0.6478 - accuracy: 0.8774 - val_loss: 2.6596 - val_accuracy: 0.6870\n",
      "Epoch 5/20\n",
      "1173/1173 [==============================] - 665s 567ms/step - loss: 0.5619 - accuracy: 0.8946 - val_loss: 2.8198 - val_accuracy: 0.6984\n",
      "Training completed in time:  0:55:34.714339\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_loss', min_delta=0.0001, patience=3)\n",
    "\n",
    "start = datetime.now()\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_path, # same directory as training data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // 32,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // 32,\n",
    "    epochs = 20,\n",
    "    callbacks=[earlystop_callback])\n",
    "\n",
    "model.save('/content/drive/MyDrive/Datasets/saved_transfer_model.h5')\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "K4Z4LrgIqrFf"
   },
   "outputs": [],
   "source": [
    "# # MobileNet\n",
    "\n",
    "# # Mobile model imported\n",
    "# MobileNetModel = MobileNet(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "\n",
    "# MobileNetModel.save('/content/drive/MyDrive/Datasets/TrialMobileNet_modelV2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OFN9AMmNnHjB"
   },
   "outputs": [],
   "source": [
    "# # Ensemble two models\n",
    "\n",
    "# keras_model = tf.keras.models.load_model('/content/drive/MyDrive/Datasets/saved_transfer_model.h5', compile=False)\n",
    "# keras_model._name = 'model1'\n",
    "# #keras_model2 = tf.keras.models.load_model('/content/drive/MyDrive/Datasets/saved_transfer_modelV2.h5', compile=False)\n",
    "# keras_model2 = tf.keras.models.load_model('/content/drive/MyDrive/Datasets/TrialMobileNet_modelV2.h5', compile=False)\n",
    "# keras_model2._name = 'model2'\n",
    "# models = [keras_model, keras_model2]\n",
    "\n",
    "# model_input = tf.keras.Input(shape=(224, 224, 3))\n",
    "# model_outputs = [model(model_input) for model in models]\n",
    "# ensemble_output = tf.keras.layers.Average()(model_outputs)\n",
    "# ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)\n",
    "\n",
    "# ensemble_model.save('/content/drive/MyDrive/Datasets/Ensembled_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xScIJpr-3z2s"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Here we import the pre-trained model we created and use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "giy3rG3q2QaA"
   },
   "outputs": [],
   "source": [
    "image_data_with_features_pkl = os.path.join('meta-data-files/','image_data_features.pkl')\n",
    "image_features_vectors_ann = os.path.join('meta-data-files/','image_features_vectors.ann')\n",
    "\n",
    "class LoadData:\n",
    "    \"\"\"Loading the data from Single/Multiple Folders or form CSV file\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def from_folder(self,folder_list:list): # Enter the Single Folder Path/List of the Folders\n",
    "        self.folder_list = folder_list\n",
    "        image_path = []\n",
    "        for folder in self.folder_list:\n",
    "            for path in os.listdir(folder):\n",
    "                image_path.append(os.path.join(folder,path))\n",
    "        return image_path # Returning list of images\n",
    "    def from_csv(self,csv_file_path:str,images_column_name:str): # CSV File path with Images path Columns Name\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.images_column_name = images_column_name\n",
    "        return pd.read_csv(self.csv_file_path)[self.images_column_name].to_list() # Returning list of images\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # Use VGG-16 as the architecture and ImageNet for the weight\n",
    "        #base_model = VGG16(weights='imagenet')\n",
    "        #base_model = keras.models.load_model('/content/saved_transfer_modelV1.h5')\n",
    "        base_model = keras.models.load_model('/content/drive/MyDrive/Datasets/Ensembled_model.h5')\n",
    "        # Customize the model to return features from fully-connected layer\n",
    "        self.model = Model(inputs=base_model.input, outputs=base_model.get_layer('dense').output)\n",
    "    def extract(self, img):\n",
    "        # Resize the image\n",
    "        img = img.resize((224, 224))\n",
    "        # Convert the image color space\n",
    "        img = img.convert('RGB')\n",
    "        # Reformat the image\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Extract Features\n",
    "        feature = self.model.predict(x)[0]\n",
    "        return feature / np.linalg.norm(feature)\n",
    "    def get_feature(self,image_data:list):\n",
    "        self.image_data = image_data \n",
    "        fe = FeatureExtractor()\n",
    "        features = []\n",
    "        for img_path in tqdm(self.image_data): # Iterate through images \n",
    "            # Extract Features\n",
    "            try:\n",
    "                feature = self.extract(img=Image.open(img_path))\n",
    "                features.append(feature)\n",
    "            except:\n",
    "                features.append(None)\n",
    "                continue\n",
    "        return features\n",
    "\n",
    "class Index:\n",
    "    def __init__(self,image_list:list):\n",
    "        self.image_list = image_list\n",
    "        if 'meta-data-files' not in os.listdir():\n",
    "            os.makedirs(\"meta-data-files\")\n",
    "        self.FE = FeatureExtractor()\n",
    "    def start_feature_extraction(self):\n",
    "        image_data = pd.DataFrame()\n",
    "        image_data['images_paths'] = self.image_list\n",
    "        f_data = self.FE.get_feature(self.image_list)\n",
    "        image_data['features']  = f_data\n",
    "        image_data = image_data.dropna().reset_index(drop=True)\n",
    "        \n",
    "        image_data.to_pickle(image_data_with_features_pkl)\n",
    "        print(\"Image Meta Information Saved: [meta-data-files/image_data_features.pkl]\")\n",
    "        return image_data\n",
    "    def start_indexing(self,image_data):\n",
    "        self.image_data = image_data\n",
    "        f = len(image_data['features'][0]) # Length of item vector that will be indexed\n",
    "        t = AnnoyIndex(f, 'euclidean')\n",
    "        for i,v in tqdm(zip(self.image_data.index,image_data['features'])):\n",
    "            t.add_item(i, v)\n",
    "        t.build(100) # 100 trees\n",
    "        print(\"Saved the Indexed File:\"+\"[meta-data-files/image_features_vectors.ann]\")\n",
    "        \n",
    "        t.save(image_features_vectors_ann)\n",
    "    def Start(self):\n",
    "        if len(os.listdir(\"meta-data-files/\"))==0:\n",
    "            data = self.start_feature_extraction()\n",
    "            self.start_indexing(data)\n",
    "        else:\n",
    "            print(\"Metadata and Features are already present, Do you want Extract Again? Enter yes or no\")\n",
    "            flag  = str(input())\n",
    "            if flag.lower() == 'yes':\n",
    "                data = self.start_feature_extraction()\n",
    "                self.start_indexing(data)\n",
    "            else:\n",
    "                print(\"Meta data already Present, Please Apply Search!\")\n",
    "                print(os.listdir(\"meta-data-files/\"))\n",
    "\n",
    "class SearchImage:\n",
    "    def __init__(self):\n",
    "        self.image_data = pd.read_pickle(image_data_with_features_pkl)\n",
    "        self.f = len(self.image_data['features'][0])\n",
    "    def search_by_vector(self,v,n:int):\n",
    "        self.v = v # Feature Vector\n",
    "        self.n = n # number of output \n",
    "        u = AnnoyIndex(self.f, 'euclidean')\n",
    "        u.load(image_features_vectors_ann) # super fast, will just mmap the file\n",
    "        index_list = u.get_nns_by_vector(self.v, self.n) # will find the 10 nearest neighbors\n",
    "        return dict(zip(index_list,self.image_data.iloc[index_list]['images_paths'].to_list()))\n",
    "    def get_query_vector(self,image_path:str):\n",
    "        self.image_path = image_path\n",
    "        img = Image.open(self.image_path)\n",
    "        fe = FeatureExtractor()\n",
    "        query_vector = fe.extract(img)\n",
    "        return query_vector\n",
    "    def plot_similar_images(self,image_path:str):\n",
    "        self.image_path = image_path\n",
    "        query_vector = self.get_query_vector(self.image_path)\n",
    "        img_list = list(self.search_by_vector(query_vector,16).values())\n",
    "        # Visualize the result\n",
    "        axes=[]\n",
    "        fig=plt.figure(figsize=(20,15))\n",
    "        for a in range(4*4):\n",
    "            axes.append(fig.add_subplot(4, 4, a+1))  \n",
    "            plt.axis('off')\n",
    "            plt.imshow(Image.open(img_list[a]))\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle('Similar Result Found', fontsize=22)\n",
    "        plt.show(fig)\n",
    "    def get_similar_images(self,image_path:str,number_of_images:int):\n",
    "        self.image_path = image_path\n",
    "        self.number_of_images = number_of_images\n",
    "        query_vector = self.get_query_vector(self.image_path)\n",
    "        img_dict = self.search_by_vector(query_vector,self.number_of_images)\n",
    "        return img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suPV4QUO6AUT"
   },
   "outputs": [],
   "source": [
    "# Unzip the gallery here:\n",
    "%%capture\n",
    "!unzip /content/drive/MyDrive/Datasets/galler.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVBX6DIJ2SrV"
   },
   "outputs": [],
   "source": [
    "# load the Images from the Folder (You can also import data from multiple folders in python list type)\n",
    "image_list = LoadData().from_folder(folder_list = ['galler'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_DU_RqV5_AD"
   },
   "outputs": [],
   "source": [
    "# For Faster Serching we need to index Data first, After Indexing all the meta data stored on the local path\n",
    "Index(image_list).Start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p04MY8dO6FP2"
   },
   "outputs": [],
   "source": [
    "# for searching, you need to give the image path and the number of the similar image you want\n",
    "top10 = SearchImage().get_similar_images(image_path='/content/A1 (7).jpg',number_of_images=10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olSxZ0-86Htt"
   },
   "outputs": [],
   "source": [
    "i =1\n",
    "for x in top10:\n",
    "  \n",
    "  print(\"Top \"+str(i)+ \" image\")\n",
    "  i=i+1\n",
    "  img = mpimg.imread(top10[x])\n",
    "  imgplot = plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjCzukrIru9N"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Ignore everything below this:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Clean slate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
